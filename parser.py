import requests, time
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import csv

#–ü–∞—Ä—Å–∏–º –∫–∞—Ç–∞–ª–æ–≥ —Ç–æ–≤–∞—Ä–æ–≤
class CatalogScraper:
    def __init__(self, url_template, total_pages, product_selector, name_selector, link_selector, price_selector, delay=1, num_start_page = 1):
        """
        :param url_template: –°—Ç—Ä–æ–∫–∞ —Å —à–∞–±–ª–æ–Ω–æ–º URL, –Ω–∞–ø—Ä–∏–º–µ—Ä: "https://site.com/catalog?page={page}"
        :param total_pages: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        :param product_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–∞
        :param name_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞ –≤–Ω—É—Ç—Ä–∏ –∫–∞—Ä—Ç–æ—á–∫–∏
        :param link_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä –≤–Ω—É—Ç—Ä–∏ –∫–∞—Ä—Ç–æ—á–∫–∏
        :param price_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä —Ü–µ–Ω—ã –≤–Ω—É—Ç—Ä–∏ –∫–∞—Ä—Ç–æ—á–∫–∏
        :param delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
        :param num_start_page: –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞ —Å –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –ø–∞—Ä—Å–∏–Ω–≥
        """
        self.url_template = url_template
        self.total_pages = total_pages
        self.product_selector = product_selector
        self.name_selector = name_selector
        self.link_selector = link_selector
        self.price_selector = price_selector
        self.delay = delay
        self.num_start_page = num_start_page
        self.base_url = "{0.scheme}://{0.netloc}".format(requests.utils.urlparse(url_template))

    def _fetch_page(self, url):
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.text

    def _parse_products(self, html):
        soup = BeautifulSoup(html, "lxml")
        products = []

        for card in soup.select(self.product_selector):
            try:
                name_elem = card.select_one(self.name_selector)
                link_elem = card.select_one(self.link_selector)
                price_elem = card.select_one(self.price_selector)

                if not (name_elem and link_elem):
                    print("–ü—Ä–æ—å–ª–µ–º–∞ —Å —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏: " + name_elem + " | " + link_elem)
                    continue

                name = name_elem.get_text(strip=True)
                rel_link = link_elem.get("href")
                link = urljoin(self.base_url, rel_link)

                price = "–¶–µ–Ω–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É"
                if price_elem is not None:
                    price = price_elem.get_text(strip=True)
                    
                products.append({
                    "name": name,
                    "url": link,
                    "price": price
                })
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ç–æ–≤–∞—Ä–∞: {e}")

        return products

    def run(self):
        all_products = []

        for page in range(self.num_start_page, self.total_pages + 1):
            page_url = self.url_template.format(page=page)
            print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {page_url}")

            try:
                html = self._fetch_page(page_url)
                products = self._parse_products(html)
                all_products.extend(products)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")

            time.sleep(self.delay)

        return all_products


#–ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã 
class URLExtractor:
    def __init__(self, data):
        self.data = data

    def extract_urls(self):
        return [item['url'] for item in self.data if 'url' in item]
    


class CharacteristicsParser:
    def __init__(self, html_dict, name_selector, value_selector):
        """
        :param html_dict: —Å–ª–æ–≤–∞—Ä—å {url: html_content}
        :param name_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        :param value_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        """
        self.html_dict = html_dict
        self.name_selector = name_selector
        self.value_selector = value_selector
        self.parsed_data = {}

    def parse_all(self):
        for url, html in self.html_dict.items():
            print(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Å {url}")
            soup = BeautifulSoup(html, "lxml")

            names = soup.select(self.name_selector)
            values = soup.select(self.value_selector)

            if len(names) != len(values):
                print(f"‚ö†Ô∏è –ù–µ—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –∏ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ {url}")
                count = min(len(names), len(values))
            else:
                count = len(names)

            characteristics = {}
            for i in range(count):
                name = names[i].get_text(strip=True)
                value = values[i].get_text(strip=True)
                if name and value:
                    characteristics[name] = value

            self.parsed_data[url] = characteristics

        return self.parsed_data
    
class ImagePreviewParser:
    def __init__(self, html_dict, img_selector):
        """
        :param html_dict: —Å–ª–æ–≤–∞—Ä—å {url: html_content}
        :param img_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, "img.product-preview")
        """
        self.html_dict = html_dict
        self.img_selector = img_selector
        self.result = {}

    def parse_all(self):
        for url, html in self.html_dict.items():
            print(f"üñº –ü–∞—Ä—Å–∏–Ω–≥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å {url}")
            soup = BeautifulSoup(html, "lxml")
            img_tag = soup.select_one(self.img_selector)

            if img_tag and img_tag.has_attr("src"):
                img_url = img_tag["src"]
                full_img_url = urljoin(url, img_url)
                self.result[url] = full_img_url
            else:
                print(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∞ {url}")
                self.result[url] = None

        return self.result
    
class ProductDataMerger:
    def __init__(self, catalog_data, characteristics, images):
        """
        :param catalog_data: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π [{'name': ..., 'url': ..., 'price': ...}, ...]
        :param characteristics: —Å–ª–æ–≤–∞—Ä—å {url: {—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞: –∑–Ω–∞—á–µ–Ω–∏–µ}}
        :param images: —Å–ª–æ–≤–∞—Ä—å {url: —Å—Å—ã–ª–∫–∞_–Ω–∞_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ}
        """
        self.catalog_data = catalog_data
        self.characteristics = characteristics
        self.images = images
        self.result = []

    def merge(self):
        for item in self.catalog_data:
            url = item.get("url")
            merged_item = {
                "URL": url,
                "–ù–∞–∑–≤–∞–Ω–∏–µ": item.get("name"),
                "–¶–µ–Ω–∞": item.get("price"),
                "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ": self.images.get(url),
            }

            # –î–æ–±–∞–≤–ª—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            specs = self.characteristics.get(url, {})
            merged_item.update(specs)

            self.result.append(merged_item)

        return self.result

    def save_to_csv(self, filepath="products.csv"):
        if not self.result:
            print("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")
            return

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        all_keys = set()
        for item in self.result:
            all_keys.update(item.keys())

        fieldnames = sorted(all_keys)

        with open(filepath, mode="w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(self.result)

        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {filepath}")