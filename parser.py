import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

#–ü–∞—Ä—Å–∏–º –∫–∞—Ç–∞–ª–æ–≥ —Ç–æ–≤–∞—Ä–æ–≤
class CatalogScraper:
    def __init__(self, url_template, total_pages, product_selector, name_selector, link_selector, price_selector, delay=1):
        """
        :param url_template: –°—Ç—Ä–æ–∫–∞ —Å —à–∞–±–ª–æ–Ω–æ–º URL, –Ω–∞–ø—Ä–∏–º–µ—Ä: "https://example.com/catalog?page={page}"
        :param total_pages: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        :param product_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä –∫–∞—Ä—Ç–æ—á–∫–∏ —Ç–æ–≤–∞—Ä–∞
        :param name_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞ –≤–Ω—É—Ç—Ä–∏ –∫–∞—Ä—Ç–æ—á–∫–∏
        :param link_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä –≤–Ω—É—Ç—Ä–∏ –∫–∞—Ä—Ç–æ—á–∫–∏
        :param price_selector: CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä —Ü–µ–Ω—ã –≤–Ω—É—Ç—Ä–∏ –∫–∞—Ä—Ç–æ—á–∫–∏
        :param delay: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (—Å–µ–∫—É–Ω–¥—ã)
        """
        self.url_template = url_template
        self.total_pages = total_pages
        self.product_selector = product_selector
        self.name_selector = name_selector
        self.link_selector = link_selector
        self.price_selector = price_selector
        self.delay = delay
        self.base_url = "{0.scheme}://{0.netloc}".format(requests.utils.urlparse(url_template))

    def _fetch_page(self, url):
        headers = {
            "User-Agent": "Mozilla/5.0"
        }
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.text

    def _parse_products(self, html):
        soup = BeautifulSoup(html, "html.parser")
        products = []

        for card in soup.select(self.product_selector):
            try:
                name_elem = card.select_one(self.name_selector)
                link_elem = card.select_one(self.link_selector)
                price_elem = card.select_one(self.price_selector)

                if not (name_elem and link_elem and price_elem):
                    continue

                name = name_elem.get_text(strip=True)
                rel_link = link_elem.get("href")
                link = urljoin(self.base_url, rel_link)
                price = price_elem.get_text(strip=True)

                products.append({
                    "name": name,
                    "url": link,
                    "price": price
                })
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Ç–æ–≤–∞—Ä–∞: {e}")
        return products

    def run(self):
        all_products = []

        for page in range(1, self.total_pages + 1):
            page_url = self.url_template.format(page=page)
            print(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {page_url}")

            try:
                html = self._fetch_page(page_url)
                products = self._parse_products(html)
                all_products.extend(products)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")

            time.sleep(self.delay)

        return all_products


#–ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã 
class URLExtractor:
    def __init__(self, data):
        self.data = data

    def extract_urls(self):
        return [item['url'] for item in self.data if 'url' in item]